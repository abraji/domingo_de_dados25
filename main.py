# -*- coding: utf-8
# Reinaldo Chaves (reichaves@gmail.com)

import os
# FOR√áA O USO DA IMPLEMENTA√á√ÉO PYTHON DO PROTOBUF PARA EVITAR CONFLITOS
# Isso √© necess√°rio porque algumas bibliotecas (como TensorFlow) podem usar implementa√ß√µes
# diferentes do Protocol Buffers, causando conflitos. For√ßar a implementa√ß√£o Python
# garante compatibilidade entre todas as bibliotecas
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

import time  # Para adicionar delays entre requisi√ß√µes e evitar rate limiting
import pandas as pd  # Para manipula√ß√£o de dados tabulares e cria√ß√£o de DataFrames
import geopandas as gpd  # Extens√£o do pandas para dados geoespaciais (shapefiles)
from dotenv import load_dotenv  # Para carregar vari√°veis de ambiente do arquivo .env
from tqdm import tqdm  # Para criar barras de progresso visuais durante processamento
import logging  # Para registrar logs estruturados do sistema
from datetime import datetime  # Para trabalhar com datas e timestamps
import random  # Para gerar delays aleat√≥rios entre requisi√ß√µes

# === IMPORTA√á√ïES DO LANGCHAIN ===
# LangChain √© um framework para construir aplica√ß√µes com LLMs (Large Language Models)

# Vectorstore para armazenar e buscar embeddings de documentos
from langchain_community.vectorstores import Chroma

# Ferramentas de busca na web
from langchain_google_community import GoogleSearchAPIWrapper  # Busca via Google Custom Search
from langchain_community.tools import DuckDuckGoSearchResults  # Busca via DuckDuckGo (fallback)

# Integra√ß√£o com Google Gemini (modelo de IA)
from langchain_google_genai import ChatGoogleGenerativeAI  # Modelo de chat do Gemini
from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Modelo de embeddings

# Utilit√°rios para processamento de texto
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divide textos longos em chunks
from langchain.chains import RetrievalQA  # Chain para Question-Answering com recupera√ß√£o
from langchain.prompts import PromptTemplate  # Template para prompts estruturados
from langchain.schema import Document  # Estrutura de dados para documentos

# Carrega as vari√°veis de ambiente do arquivo .env
# Isso inclui API keys do Google, credenciais, etc.
load_dotenv()

# === CONFIGURA√á√ÉO DO SISTEMA DE LOGGING ===
# Define formato e n√≠vel de logging para toda a aplica√ß√£o
# INFO: mostra mensagens informativas gerais
# Format: timestamp - n√≠vel - mensagem
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)  # Cria logger espec√≠fico para este m√≥dulo

# === CONFIGURA√á√ïES GLOBAIS DO SISTEMA ===
# Caminhos e nomes de arquivos utilizados em todo o script

# Caminho para o shapefile com dados do SIGMINE (Sistema de Informa√ß√µes Geogr√°ficas da Minera√ß√£o)
SHAPEFILE_PATH = "data/BRASIL/BRASIL.shp"
'''
Fonte dos dados:
https://dados.gov.br/dados/conjuntos-dados/sistema-de-informacoes-geograficas-da-mineracao-sigmine
Arquivo de Metadados
Processos miner√°rios ativos - Brasil
'''

# Diret√≥rio onde ser√£o salvos os resultados
OUTPUT_DIR = "output"

# Nome do arquivo do relat√≥rio final em Markdown
REPORT_FILENAME = os.path.join(OUTPUT_DIR, "relatorio_sigmine_contexto.md")

# === NOMES DAS COLUNAS DO SHAPEFILE ===
# Define os nomes das colunas que ser√£o usadas do shapefile
# Isso facilita manuten√ß√£o caso os nomes mudem
COL_PROCESSO = "PROCESSO"  # N√∫mero do processo miner√°rio (ex: 803237/2022)
COL_TITULAR = "NOME"       # Nome da empresa titular do processo
COL_UF = "UF"             # Unidade Federativa (estado)

# Vari√°vel global para rastrear qual motor de busca foi efetivamente utilizado
# Ser√° preenchida em runtime com "Google Search API" ou "DuckDuckGo Search"
SEARCH_ENGINE_USED = None

def setup_search_tool():
    """
    Configura a ferramenta de busca na web, priorizando Google Search API.
    
    Esta fun√ß√£o tenta primeiro usar o Google Search (mais preciso e estruturado),
    mas se falhar ou n√£o estiver configurado, usa DuckDuckGo como fallback.
    
    Returns:
        search_tool: Inst√¢ncia de GoogleSearchAPIWrapper ou DuckDuckGoSearchResults
    
    A escolha √© importante porque:
    - Google Search retorna resultados estruturados (t√≠tulo, link, snippet)
    - DuckDuckGo retorna texto n√£o estruturado que precisa ser parseado
    """
    global SEARCH_ENGINE_USED  # Permite modificar a vari√°vel global
    
    # Tenta buscar as credenciais do Google no arquivo .env
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")  # Custom Search Engine ID

    # Se ambas as credenciais existem, tenta configurar Google Search
    if google_api_key and google_cse_id:
        try:
            # Cria inst√¢ncia da ferramenta Google Search
            search_tool = GoogleSearchAPIWrapper()
            
            # Testa se a ferramenta funciona fazendo uma busca simples
            test_results = search_tool.run("test query")
            
            # Se retornou resultados, a configura√ß√£o est√° correta
            if test_results:
                print("‚úÖ Usando Google Search como ferramenta de busca.")
                SEARCH_ENGINE_USED = "Google Search API"
                return search_tool
                
        except Exception as e:
            # Se houver erro, registra mas continua para o fallback
            logger.warning(f"Google Search API configurada mas falhou: {e}")
    
    # Fallback: usa DuckDuckGo que n√£o precisa de API key
    print("‚ö†Ô∏è Usando DuckDuckGoSearchResults como ferramenta de busca.")
    SEARCH_ENGINE_USED = "DuckDuckGo Search"
    return DuckDuckGoSearchResults()

def extract_urls_from_duckduckgo_text(text):
    """
    Extrai URLs de texto n√£o estruturado retornado pelo DuckDuckGo.
    
    O DuckDuckGo retorna resultados como texto plano, ent√£o precisamos
    usar regex para encontrar e extrair as URLs.
    
    Args:
        text (str): Texto bruto retornado pelo DuckDuckGo
        
    Returns:
        list: Lista de URLs √∫nicas encontradas no texto
        
    Exemplo:
        Input: "Resultado sobre minera√ß√£o... https://exemplo.com/noticia..."
        Output: ["https://exemplo.com/noticia"]
    """
    import re
    
    # Padr√£o regex para capturar URLs HTTP/HTTPS
    # Captura: protocolo://dominio/caminho
    # Exclui caracteres que normalmente delimitam URLs em texto
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:\.[^\s<>"{}|\\^`\[\]]+)*'
    
    # Encontra todas as URLs que correspondem ao padr√£o
    urls = re.findall(url_pattern, text)
    
    # Remove duplicatas mantendo a ordem original
    seen = set()  # Conjunto para rastrear URLs j√° vistas
    unique_urls = []
    
    for url in urls:
        # Ignora URLs que terminam com '.' (comum em fins de frase)
        if url not in seen and not url.endswith('.'):
            seen.add(url)
            unique_urls.append(url)
            
    return unique_urls

def enhanced_search(titular: str, processo: str, uf: str, search_tool):
    """
    Realiza busca aprimorada na web com m√∫ltiplas estrat√©gias e Google Dorks.
    
    Esta √© uma das fun√ß√µes mais importantes do sistema. Ela implementa
    tr√™s estrat√©gias de busca para maximizar as chances de encontrar
    informa√ß√µes relevantes sobre impactos socioambientais.
    
    Args:
        titular (str): Nome da empresa titular do processo
        processo (str): N√∫mero do processo (ex: "803237/2022")
        uf (str): Estado (sigla)
        search_tool: Ferramenta de busca configurada
        
    Returns:
        list: Lista de dicion√°rios com resultados de busca estruturados
        
    Estrat√©gias implementadas:
    1. Buscas b√°sicas: termos gerais sobre a empresa
    2. Buscas de impacto: termos espec√≠ficos sobre conflitos
    3. Buscas direcionadas: usando Google Dorks em sites especializados
    """
    all_results = []  # Lista para acumular todos os resultados
    
    # Remove a barra do n√∫mero do processo para algumas buscas
    # Ex: "803237/2022" -> "8032372022"
    processo_clean = processo.replace('/', '')
    
    # Lista de sites especializados em quest√µes socioambientais e minera√ß√£o
    # Estes sites s√£o priorizados porque tendem a ter informa√ß√µes mais confi√°veis
    sites_relevantes = [
        "terrasindigenas.org.br",    # Monitoramento de terras ind√≠genas
        "mpf.mp.br",                 # Minist√©rio P√∫blico Federal
        "ibama.gov.br",              # Instituto Brasileiro do Meio Ambiente
        "funai.gov.br",              # Funda√ß√£o Nacional do √çndio
        "socioambiental.org",        # Instituto Socioambiental
        "cimi.org.br",               # Conselho Indigenista Mission√°rio
        "imazon.org.br",             # Instituto do Homem e Meio Ambiente da Amaz√¥nia
        "inesc.org.br",              # Instituto de Estudos Socioecon√¥micos
        "apublica.org",              # Ag√™ncia de jornalismo investigativo
        "reporterbrasil.org.br"      # ONG de jornalismo socioambiental
    ]
    
    # === ESTRAT√âGIA 1: BUSCAS B√ÅSICAS ===
    # Termos mais gerais para capturar informa√ß√µes gerais sobre a empresa/processo
    basic_searches = [
        f'"{titular}" {uf}',           # Busca exata da empresa + estado
        f'"{titular}" minera√ß√£o',      # Empresa + contexto de minera√ß√£o
        f'processo {processo}',        # Busca direta pelo n√∫mero do processo
        f'ANM {processo}',            # ANM = Ag√™ncia Nacional de Minera√ß√£o
        f'SIGMINE {processo_clean}',  # SIGMINE + processo sem barra
    ]
    
    # === ESTRAT√âGIA 2: BUSCAS COM TERMOS DE IMPACTO ===
    # Termos espec√≠ficos para encontrar potenciais conflitos e impactos
    impact_searches = [
        f'"{titular}" terra ind√≠gena',         # Conflitos com TIs
        f'"{titular}" comunidade tradicional', # Impactos em comunidades
        f'"{titular}" impacto ambiental',     # Estudos de impacto
        f'"{titular}" conflito socioambiental', # Conflitos gerais
        f'"{titular}" a√ß√£o civil p√∫blica',     # A√ß√µes judiciais
        f'processo {processo} impacto',       # Impactos do processo espec√≠fico
        f'processo {processo} terra ind√≠gena', # Processo em TIs
    ]
    
    # === ESTRAT√âGIA 3: BUSCAS COM GOOGLE DORKS ===
    # Usa operador "site:" para buscar diretamente em sites especializados
    site_searches = []
    for site in sites_relevantes[:5]:  # Limita a 5 sites para n√£o sobrecarregar
        site_searches.extend([
            f'site:{site} "{titular}"',        # Empresa no site espec√≠fico
            f'site:{site} {processo}',         # Processo com barra
            f'site:{site} {processo_clean}'    # Processo sem barra
        ])
    
    # Combina estrat√©gias limitando quantidade para evitar rate limiting
    # Total: 3 b√°sicas + 3 de impacto + 6 em sites = 12 buscas
    all_searches = basic_searches[:3] + impact_searches[:3] + site_searches[:6]
    
    print(f"\n  üìç Executando {len(all_searches)} buscas estrat√©gicas...")
    
    # Executa cada busca com tratamento de erros e rate limiting
    for idx, search_query in enumerate(all_searches):
        try:
            # Adiciona delay aleat√≥rio entre buscas (exceto na primeira)
            # Isso evita ser bloqueado por fazer muitas requisi√ß√µes r√°pidas
            if idx > 0:
                time.sleep(random.uniform(1.5, 2.5))  # Entre 1.5 e 2.5 segundos
            
            # Executa a busca
            results = search_tool.run(search_query)
            
            # === PROCESSAMENTO PARA DUCKDUCKGO ===
            if isinstance(results, str) and results:
                # DuckDuckGo retorna string n√£o estruturada
                
                # Tenta extrair URLs do texto
                urls_found = extract_urls_from_duckduckgo_text(results)
                
                if urls_found:
                    # Se encontrou URLs, cria uma entrada para cada uma
                    for url in urls_found[:3]:  # M√°ximo 3 URLs por busca
                        all_results.append({
                            'content': results[:500],  # Primeiros 500 caracteres
                            'query': search_query,     # Query que gerou o resultado
                            'link': url,              # URL extra√≠da
                            'source': url,            # Duplica para compatibilidade
                            'title': f'Resultado de {search_query}',
                            'strategy': 'duckduckgo_extracted',
                            # Verifica se √© de um site relevante
                            'is_relevant_site': any(site in url for site in sites_relevantes)
                        })
                else:
                    # Se n√£o encontrou URLs, salva s√≥ o conte√∫do
                    all_results.append({
                        'content': results[:500],
                        'query': search_query,
                        'source': f'Busca: {search_query}',
                        'link': '',  # Sem URL
                        'title': 'Resultado sem URL extra√≠da',
                        'strategy': 'text_result',
                        'is_relevant_site': False
                    })
                    
            # === PROCESSAMENTO PARA GOOGLE SEARCH ===
            elif isinstance(results, list):
                # Google retorna lista de dicion√°rios estruturados
                for item in results:
                    if isinstance(item, dict):
                        link = item.get('link', '')
                        
                        # Verifica se o link √© de um site relevante
                        is_relevant_site = any(site in link for site in sites_relevantes)
                        
                        all_results.append({
                            'content': item.get('snippet', ''),  # Trecho do resultado
                            'title': item.get('title', ''),      # T√≠tulo da p√°gina
                            'link': link,                        # URL
                            'source': link,                      # Duplica para compatibilidade
                            'query': search_query,               # Query usada
                            'is_relevant_site': is_relevant_site,
                            'strategy': 'structured_result'
                        })
                        
        except Exception as e:
            # Tratamento espec√≠fico para rate limiting (erro 429)
            if "429" in str(e):
                print(f"  ‚ö†Ô∏è Rate limit atingido. Aguardando 5 segundos...")
                time.sleep(5)
            else:
                # Outros erros s√£o logados mas n√£o interrompem o processo
                logger.warning(f"  ‚ö†Ô∏è Erro na busca '{search_query}': {e}")
            continue
    
    # Ordena resultados priorizando sites relevantes
    # Sites como MPF, FUNAI, etc. aparecem primeiro
    all_results.sort(key=lambda x: x.get('is_relevant_site', False), reverse=True)
    
    print(f"  ‚úÖ {len(all_results)} resultados encontrados")
    return all_results

def rag_summary_enhanced(query: str, search_tool, llm, embed_model, titular: str, processo: str, uf: str):
    """
    Implementa um sistema RAG (Retrieval-Augmented Generation) aprimorado.
    
    RAG combina busca de informa√ß√µes (Retrieval) com gera√ß√£o de texto (Generation)
    para criar resumos contextualizados baseados em fontes reais.
    
    Args:
        query (str): Query inicial de busca
        search_tool: Ferramenta de busca configurada
        llm: Modelo de linguagem (Gemini)
        embed_model: Modelo para criar embeddings de texto
        titular (str): Nome da empresa
        processo (str): N√∫mero do processo
        uf (str): Estado
        
    Returns:
        dict: Dicion√°rio com resumo, fontes e descobertas relevantes
        
    Fluxo:
    1. Busca informa√ß√µes na web
    2. Cria embeddings dos documentos encontrados
    3. Usa o LLM para analisar e resumir com base no contexto
    4. Extrai e organiza as fontes citadas
    """
    print(f"\nüîç Analisando: {processo} - {titular} ({uf})")
    
    # Executa busca aprimorada com todas as estrat√©gias
    search_results = enhanced_search(titular, processo, uf, search_tool)
    
    # Verifica se encontrou resultados
    if not search_results:
        return {
            'summary': "Nenhuma informa√ß√£o encontrada na web para esta consulta.",
            'sources': [],
            'raw_findings': []
        }
    
    # === CRIA√á√ÉO DE DOCUMENTOS PARA O RAG ===
    docs = []  # Lista de documentos LangChain
    source_mapping = {}  # Mapeia IDs para resultados originais
    
    # Converte cada resultado de busca em um Document do LangChain
    for idx, result in enumerate(search_results):
        content = result.get('content', '')
        if content:
            doc_id = f"doc_{idx}"  # ID √∫nico para o documento
            
            # Cria documento com metadados completos
            docs.append(Document(
                page_content=content,  # Conte√∫do textual
                metadata={
                    'doc_id': doc_id,
                    'source': result.get('link', result.get('source', 'Fonte n√£o especificada')),
                    'title': result.get('title', ''),
                    'query': result.get('query', ''),
                    'link': result.get('link', '')  # URL quando dispon√≠vel
                }
            ))
            source_mapping[doc_id] = result
    
    # Verifica se h√° documentos para processar
    if not docs:
        return {
            'summary': "Os resultados da busca n√£o continham conte√∫do process√°vel.",
            'sources': [],
            'raw_findings': []
        }
    
    # === DIVIS√ÉO DE DOCUMENTOS EM CHUNKS ===
    # Documentos muito longos s√£o divididos em peda√ßos menores
    # para melhor processamento pelo modelo de embeddings
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,     # Tamanho m√°ximo de cada chunk
        chunk_overlap=200    # Sobreposi√ß√£o entre chunks para manter contexto
    )
    docs_split = splitter.split_documents(docs)
    
    # === CRIA√á√ÉO DA BASE VETORIAL ===
    # Converte texto em vetores num√©ricos (embeddings) para busca sem√¢ntica
    vect = Chroma.from_documents(docs_split, embed_model)
    
    # === TEMPLATE DO PROMPT ===
    # Define como o LLM deve analisar e estruturar a resposta
    enhanced_prompt_template = """
    Voc√™ √© um analista especializado em minera√ß√£o e impactos socioambientais. 
    Analise cuidadosamente o contexto fornecido sobre o processo miner√°rio.
    
    **INSTRU√á√ïES IMPORTANTES:**
    1. Extraia TODAS as informa√ß√µes relevantes do contexto, especialmente:
       - Tipo de min√©rio e subst√¢ncias
       - Status atual do projeto (pesquisa, lavra, etc.)
       - Localiza√ß√£o espec√≠fica (munic√≠pio, coordenadas se dispon√≠vel)
       - QUALQUER men√ß√£o a impactos socioambientais, incluindo:
         * Sobreposi√ß√£o com Terras Ind√≠genas (CITE O NOME DA TI)
         * Conflitos com comunidades tradicionais
         * Quest√µes ambientais (desmatamento, polui√ß√£o, etc.)
         * A√ß√µes do Minist√©rio P√∫blico
         * Multas ou san√ß√µes ambientais
         * Acidentes ou incidentes
         * Protestos ou manifesta√ß√µes
    
    2. Para cada informa√ß√£o importante, indique de qual documento ela veio usando [Fonte: doc_X]
    
    3. Se encontrar informa√ß√µes sobre terras ind√≠genas, comunidades afetadas ou impactos ambientais, 
       descreva-os em detalhes, n√£o apenas mencione sua exist√™ncia.
    
    **Contexto dispon√≠vel:**
    {context}
    
    **Pergunta:** {question}
    
    **Resposta estruturada com cita√ß√£o das fontes:**
    """
    
    # Cria template com as vari√°veis necess√°rias
    PROMPT = PromptTemplate(
        template=enhanced_prompt_template,
        input_variables=["context", "question"]
    )
    
    # === CONFIGURA√á√ÉO DA CADEIA DE QA ===
    # RetrievalQA combina recupera√ß√£o de documentos com gera√ß√£o de resposta
    qa_chain = RetrievalQA.from_chain_type(
        llm,  # Modelo Gemini
        retriever=vect.as_retriever(search_kwargs={"k": 8}),  # Busca top-8 chunks mais relevantes
        chain_type="stuff",  # M√©todo que passa todos os docs de uma vez
        chain_type_kwargs={
            "prompt": PROMPT,
            "verbose": False  # N√£o mostra logs internos
        },
        return_source_documents=True  # Retorna os documentos usados na resposta
    )
    
    # === EXECU√á√ÉO DA AN√ÅLISE ===
    # Invoca a cadeia com a pergunta espec√≠fica sobre o processo
    resposta = qa_chain.invoke({
        "query": f"Analise todas as informa√ß√µes sobre o processo {processo} da {titular}, especialmente impactos socioambientais"
    })
    
    # === EXTRA√á√ÉO DE FONTES √öNICAS ===
    # Processa os documentos citados para extrair URLs √∫nicas
    sources_with_metadata = []
    seen_urls = set()  # Evita duplicatas
    
    if 'source_documents' in resposta:
        for doc in resposta['source_documents']:
            # Tenta pegar o link dos metadados
            url = doc.metadata.get('link', '') or doc.metadata.get('source', '')
            
            # Adiciona apenas URLs v√°lidas e √∫nicas
            if url and url not in seen_urls and url.startswith('http'):
                seen_urls.add(url)
                sources_with_metadata.append({
                    'url': url,
                    'title': doc.metadata.get('title', ''),
                    'query': doc.metadata.get('query', '')
                })
    
    # === PROCESSAMENTO DE DESCOBERTAS RELEVANTES ===
    # Identifica e pontua resultados com base em palavras-chave de impacto
    raw_findings = []
    
    # Palavras-chave que indicam potenciais impactos socioambientais
    keywords = [
        'terra ind√≠gena', 'conflito', 'amea√ßa', 'impacto', 'ambiental', 
        'comunidade', 'protesto', 'multa', 'a√ß√£o civil', 'minist√©rio p√∫blico',
        'sobreposi√ß√£o', 'desmatamento', 'polui√ß√£o', 'contamina√ß√£o'
    ]
    
    # Analisa cada resultado e atribui pontua√ß√£o de relev√¢ncia
    for result in search_results:
        content_lower = result.get('content', '').lower()
        
        # Sistema de pontua√ß√£o
        score = 0
        
        # +10 pontos se for de site relevante (MPF, FUNAI, etc.)
        if result.get('is_relevant_site', False):
            score += 10
            
        # +1 ponto para cada palavra-chave encontrada
        score += sum(1 for keyword in keywords if keyword in content_lower)
        
        # Adiciona apenas resultados com pontua√ß√£o > 0
        if score > 0:
            result['relevance_score'] = score
            raw_findings.append(result)
    
    # Ordena por relev√¢ncia (maior pontua√ß√£o primeiro)
    raw_findings.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    # Retorna estrutura completa com resumo, fontes e descobertas
    return {
        'summary': resposta['result'],  # Resumo gerado pelo LLM
        'sources': sources_with_metadata,  # URLs citadas
        'raw_findings': raw_findings[:5]  # Top-5 descobertas mais relevantes
    }

def format_report_section(processo: str, data: dict, row_data: dict):
    """
    Formata uma se√ß√£o do relat√≥rio Markdown para um processo espec√≠fico.
    
    Esta fun√ß√£o √© respons√°vel pela apresenta√ß√£o visual dos dados,
    organizando as informa√ß√µes de forma clara e hier√°rquica.
    
    Args:
        processo (str): N√∫mero do processo
        data (dict): Dados da an√°lise (resumo, fontes, descobertas)
        row_data (dict): Dados originais do shapefile
        
    Returns:
        str: Se√ß√£o formatada em Markdown
    """
    # === CABE√áALHO DA SE√á√ÉO ===
    section = f"### üìã Processo {processo}\n\n"
    
    # Informa√ß√µes b√°sicas do processo
    section += f"**Titular:** {row_data[COL_TITULAR]}\n"
    section += f"**UF:** {row_data[COL_UF]}\n"
    section += f"**√Årea:** {row_data['area_ha_calculada']:.2f} hectares\n\n"
    
    # === RESUMO DA AN√ÅLISE ===
    section += "#### üìä An√°lise do Contexto:\n"
    section += f"{data['summary'].strip()}\n\n"
    
    # === DESCOBERTAS SOBRE IMPACTOS ===
    if data.get('raw_findings'):
        section += "#### ‚ö†Ô∏è Descobertas Relevantes sobre Impactos:\n"
        
        # Lista at√© 5 descobertas mais relevantes
        for idx, finding in enumerate(data['raw_findings'][:5], 1):
            section += f"\n**Descoberta {idx}:**\n"
            
            # Conte√∫do da descoberta (limitado a 500 caracteres)
            section += f"> {finding.get('content', '')[:500]}...\n"
            
            # Fonte da descoberta
            source = finding.get('link', finding.get('source', ''))
            if source and source.startswith('http'):
                # Se tem URL v√°lida, cria link clic√°vel
                title = finding.get('title', 'Link')
                section += f"> *Fonte: [{title}]({source})*\n"
                
                # Indica se √© de site especializado
                if finding.get('is_relevant_site', False):
                    section += f"> üéØ **Site especializado em impactos socioambientais**\n"
            else:
                # Se n√£o tem URL, mostra a query usada
                section += f"> *Fonte: Busca via {finding.get('query', 'consulta web')}*\n"
    
    # === LISTA DE FONTES CONSULTADAS ===
    if data.get('sources'):
        section += "\n#### üîó Fontes Consultadas:\n"
        seen_urls = set()  # Evita URLs duplicadas
        valid_sources_count = 0
        
        for source in data['sources']:
            url = source.get('url', '')
            title = source.get('title', '')
            query = source.get('query', '')
            
            # Processa apenas URLs v√°lidas e √∫nicas
            if url and url not in seen_urls and url.startswith('http'):
                seen_urls.add(url)
                valid_sources_count += 1
                
                # Se n√£o tem t√≠tulo, usa o dom√≠nio
                if not title or title == 'Fonte':
                    # Extrai dom√≠nio da URL (ex: www.exemplo.com)
                    title = url.split('/')[2] if len(url.split('/')) > 2 else 'Link'
                
                # Formata como link Markdown
                section += f"- [{title}]({url})"
                
                # Adiciona query se dispon√≠vel
                if query:
                    section += f" *(busca: {query})*"
                section += "\n"
        
        # Se n√£o encontrou URLs v√°lidas, adiciona nota explicativa
        if valid_sources_count == 0:
            section += f"*Nota: As buscas foram realizadas via {SEARCH_ENGINE_USED} mas as URLs espec√≠ficas n√£o puderam ser extra√≠das.*\n"
    
    # Separador entre se√ß√µes
    section += "\n---\n\n"
    return section

def main():
    """
    Fun√ß√£o principal que orquestra todo o processo de an√°lise.
    
    Esta fun√ß√£o coordena todas as etapas do pipeline:
    1. Leitura do shapefile
    2. Sele√ß√£o dos top-10 processos por √°rea
    3. Busca de informa√ß√µes na web
    4. An√°lise com IA
    5. Gera√ß√£o de relat√≥rios
    
    O fluxo √© projetado para ser robusto, com tratamento de erros
    e feedback visual do progresso.
    """
    # Banner inicial
    print("üöÄ INICIANDO AN√ÅLISE APRIMORADA DE CONTEXTO SIGMINE")
    print("=" * 60)

    # === ETAPA 1: LEITURA E PROCESSAMENTO DO SHAPEFILE ===
    try:
        print(f"\nüìÅ 1. Lendo shapefile de: {SHAPEFILE_PATH}")
        
        # L√™ o shapefile usando GeoPandas
        # GeoPandas estende pandas para trabalhar com dados geoespaciais
        sig = gpd.read_file(SHAPEFILE_PATH)
        print("   ‚úÖ Shapefile lido com sucesso.")

        # === REPROJE√á√ÉO DO SISTEMA DE COORDENADAS ===
        # Converte para SIRGAS 2000 / Brazil Polyconic (EPSG:5880)
        # Isso √© necess√°rio para calcular √°reas em metros quadrados
        # Sistemas de coordenadas geogr√°ficas (lat/lon) n√£o permitem
        # c√°lculos de √°rea precisos
        sig = sig.to_crs(5880)
        
        # === C√ÅLCULO DA √ÅREA EM HECTARES ===
        # GeoPandas calcula √°rea em m¬≤ para proje√ß√µes m√©tricas
        # Dividimos por 10.000 para converter m¬≤ em hectares
        sig["area_ha_calculada"] = sig.area / 10_000
        
        # === SELE√á√ÉO DOS TOP-10 PROCESSOS ===
        # Seleciona os 10 maiores processos miner√°rios por √°rea
        # Isso foca a an√°lise nos processos mais significativos
        top10 = sig.nlargest(10, "area_ha_calculada").copy()
        
        # === AN√ÅLISE DE FREQU√äNCIA DE TITULARES ===
        # Identifica empresas que aparecem m√∫ltiplas vezes no top-10
        # Isso pode indicar grandes players no setor
        freq = (top10.groupby([COL_TITULAR], as_index=False)
                    .size()  # Conta ocorr√™ncias
                    .sort_values("size", ascending=False))  # Ordena por frequ√™ncia
        
        # Filtra apenas titulares com mais de uma ocorr√™ncia
        freq_mais_1 = freq.query("size > 1")

        # === EXIBI√á√ÉO DOS RESULTADOS PRELIMINARES ===
        print("\nüìä Top-10 processos por √°rea (em hectares):")
        # Mostra tabela com colunas selecionadas
        print(top10[[COL_PROCESSO, "area_ha_calculada", COL_TITULAR, COL_UF]].to_string())
        
        # Se houver titulares repetidos, mostra
        if not freq_mais_1.empty:
            print("\nüè¢ Titulares que aparecem mais de uma vez no Top-10:")
            print(freq_mais_1.to_string())

    except Exception as e:
        # Tratamento de erros na leitura do shapefile
        # Erros comuns: arquivo n√£o encontrado, formato inv√°lido, colunas faltando
        print(f"‚ùå ERRO CR√çTICO ao ler ou processar o shapefile: {e}")
        return  # Encerra o programa se n√£o conseguir ler os dados

    # === ETAPA 2: CONFIGURA√á√ÉO DAS FERRAMENTAS DE IA ===
    print("\nü§ñ 2. Configurando ferramentas de IA e busca...")
    
    # Configura ferramenta de busca (Google ou DuckDuckGo)
    search_tool = setup_search_tool()
    
    # === CONFIGURA√á√ÉO DO MODELO GEMINI ===
    # Gemini 2.5 Pro: modelo mais recente e capaz do Google
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",  # Vers√£o do modelo
        temperature=0.1,         # Baixa temperatura = respostas mais consistentes
        max_output_tokens=2048   # Limite de tokens na resposta
    )
    
    # Modelo de embeddings para vetoriza√ß√£o de texto
    # Usado para busca sem√¢ntica nos documentos
    embed_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # === ETAPA 3: BUSCA E AN√ÅLISE DE CONTEXTO EXTERNO ===
    print("\nüîç 3. Iniciando busca aprimorada de contexto externo...")
    print("   Isso pode levar alguns minutos...")
    
    # Dicion√°rio para armazenar contexto de cada processo
    contexto_proc = {}
    
    # === LOOP PRINCIPAL: AN√ÅLISE DE CADA PROCESSO ===
    # tqdm cria uma barra de progresso visual
    for _, row in tqdm(top10.iterrows(), total=len(top10), desc="Analisando Processos Top-10"):
        # Extrai dados do processo atual
        cod = row[COL_PROCESSO]      # Ex: "803237/2022"
        titular = row[COL_TITULAR]   # Ex: "VALE S.A."
        uf = row[COL_UF]            # Ex: "PA"
        
        # Query inicial simplificada
        # N√£o precisa ser complexa pois enhanced_search far√° varia√ß√µes
        busca_inteligente = f'"{titular}" {uf}'
        
        # Executa an√°lise RAG completa para o processo
        contexto_proc[cod] = rag_summary_enhanced(
            busca_inteligente,  # Query base
            search_tool,        # Ferramenta de busca
            llm,               # Modelo Gemini
            embed_model,       # Modelo de embeddings
            titular,           # Nome da empresa
            cod,               # N√∫mero do processo
            uf                 # Estado
        )
        
        # Adiciona dados originais do shapefile (convertidos para dict)
        # Necess√°rio para serializa√ß√£o posterior
        contexto_proc[cod]['row_data'] = row.to_dict()

    # === ETAPA 4: AN√ÅLISE DE TITULARES RECORRENTES ===
    # Se houver empresas que aparecem m√∫ltiplas vezes
    contexto_emp = {}
    if not freq_mais_1.empty:
        print("\nüè¢ 4. Analisando perfil de titulares recorrentes...")
        
        # An√°lise espec√≠fica para cada empresa recorrente
        for _, row in tqdm(freq_mais_1.iterrows(), total=len(freq_mais_1), desc="Analisando Titulares"):
            nome = row[COL_TITULAR]
            
            # Busca mais ampla sobre o perfil da empresa
            busca_empresa = f'"{nome}" mineradora perfil ambiental conflitos comunidades ind√≠genas'
            
            contexto_emp[nome] = rag_summary_enhanced(
                busca_empresa,
                search_tool,
                llm,
                embed_model,
                nome,
                "Perfil Empresarial",  # Tipo gen√©rico
                "Brasil"               # UF gen√©rica
            )

    # === ETAPA 5: GERA√á√ÉO DO RELAT√ìRIO FINAL ===
    print("\nüìù 5. Gerando relat√≥rio final aprimorado...")
    
    # Cria diret√≥rio de sa√≠da se n√£o existir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # === CONSTRU√á√ÉO DO RELAT√ìRIO MARKDOWN ===
    # Markdown √© escolhido por ser leg√≠vel, version√°vel e convert√≠vel
    relatorio_md = f"""# üìä Relat√≥rio SIGMINE ‚Äì An√°lise Aprimorada de Contexto com IA
    
**Data de Gera√ß√£o:** {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}  
**Modelo IA:** Google Gemini  
**Processos Analisados:** {len(top10)}  

---

## üéØ Resumo Executivo

Este relat√≥rio apresenta uma an√°lise detalhada dos {len(top10)} maiores processos miner√°rios 
identificados no shapefile SIGMINE, com foco especial em impactos socioambientais, 
sobreposi√ß√µes com terras ind√≠genas e conflitos com comunidades.

### ‚ö†Ô∏è Alertas Principais:
"""
    
    # === IDENTIFICA√á√ÉO DE PROCESSOS COM IMPACTOS ===
    # Analisa o resumo de cada processo procurando palavras-chave
    processos_com_impacto = []
    for cod, data in contexto_proc.items():
        summary_lower = data['summary'].lower()
        
        # Palavras que indicam potenciais impactos
        termos_impacto = ['terra ind√≠gena', 'conflito', 'impacto', 'amea√ßa']
        
        # Se encontrar qualquer termo, marca como processo com impacto
        if any(termo in summary_lower for termo in termos_impacto):
            processos_com_impacto.append(cod)
    
    # Adiciona alertas ao resumo executivo
    if processos_com_impacto:
        relatorio_md += f"\n- **{len(processos_com_impacto)} processos** com poss√≠veis impactos socioambientais identificados\n"
        
        # Lista os 3 primeiros como exemplo
        for proc in processos_com_impacto[:3]:
            relatorio_md += f"  - Processo {proc}: Ver an√°lise detalhada abaixo\n"
    else:
        relatorio_md += "\n- Nenhum impacto socioambiental significativo foi identificado nas fontes consultadas\n"
    
    # === SE√á√ÉO 1: AN√ÅLISE DETALHADA DOS PROCESSOS ===
    relatorio_md += "\n---\n\n## üìã 1. An√°lise Detalhada dos Processos Top-10\n\n"
    
    # Adiciona uma se√ß√£o para cada processo
    for cod, data in contexto_proc.items():
        relatorio_md += format_report_section(cod, data, data['row_data'])

    # === SE√á√ÉO 2: PERFIL DOS TITULARES RECORRENTES ===
    if contexto_emp:
        relatorio_md += "\n## üè¢ 2. Perfil dos Titulares Recorrentes\n\n"
        
        for nome, data in contexto_emp.items():
            relatorio_md += f"### {nome}\n\n"
            relatorio_md += f"{data['summary'].strip()}\n\n"
            
            # Lista fontes consultadas
            if data.get('sources'):
                relatorio_md += "**Fontes Consultadas:**\n"
                seen_urls = set()  # Evita duplicatas
                
                for source in data['sources']:
                    url = source.get('url', '')
                    title = source.get('title', 'Fonte')
                    
                    if url and url not in seen_urls and url.startswith('http'):
                        seen_urls.add(url)
                        relatorio_md += f"- [{title}]({url})\n"
                        
            relatorio_md += "\n---\n"

    # === NOTAS METODOL√ìGICAS ===
    # Importante para transpar√™ncia e reprodutibilidade
    relatorio_md += f"""
## üìå Notas Metodol√≥gicas

- **Fonte dos dados espaciais:** Shapefile SIGMINE
- **Ferramentas de busca:** {SEARCH_ENGINE_USED}
- **Modelo de IA:** Google Gemini 2.5
- **Palavras-chave utilizadas:** minera√ß√£o, impacto ambiental, terra ind√≠gena, conflito, comunidade

### üìÑ Refer√™ncias dos Documentos

As cita√ß√µes no formato [Fonte: doc_X] referem-se aos documentos recuperados durante a busca na web. Cada "doc_X" representa um trecho espec√≠fico de conte√∫do encontrado nas fontes consultadas. O mapeamento dos documentos √©:

- **doc_1 a doc_N**: Trechos dos resultados de busca, ordenados pela relev√¢ncia e pela presen√ßa em sites especializados
- **Sites priorit√°rios**: terrasindigenas.org.br, mpf.mp.br, ibama.gov.br, funai.gov.br
- **Estrat√©gias de busca**: Buscas b√°sicas, buscas com termos de impacto, e buscas direcionadas a sites espec√≠ficos

Para verificar a origem exata de cada cita√ß√£o:
1. Consulte a se√ß√£o "Fontes Consultadas" em cada processo para ver as URLs dos sites pesquisados
2. O arquivo CSV "descobertas_impactos_detalhadas.csv" cont√©m o conte√∫do completo de cada descoberta com sua respectiva fonte
3. Os n√∫meros dos documentos s√£o atribu√≠dos sequencialmente conforme os resultados s√£o processados

### ‚ö†Ô∏è Limita√ß√µes:
- As informa√ß√µes s√£o baseadas em fontes p√∫blicas dispon√≠veis na internet
- A aus√™ncia de men√ß√£o a impactos n√£o significa que eles n√£o existam
- Recomenda-se verifica√ß√£o adicional com fontes oficiais (FUNAI, IBAMA, MPF)
- As refer√™ncias [Fonte: doc_X] s√£o internas ao sistema de processamento e podem variar entre execu√ß√µes

---
*Relat√≥rio gerado automaticamente por sistema de an√°lise SIGMINE com IA*
"""

    # === SALVAMENTO DO RELAT√ìRIO MARKDOWN ===
    with open(REPORT_FILENAME, "w", encoding="utf-8") as f:
        f.write(relatorio_md)

    # === GERA√á√ÉO DO CSV DE RESULTADOS ===
    # Cria estrutura tabular para an√°lise em Excel/pandas
    csv_data = []
    
    for cod, data in contexto_proc.items():
        row_info = data.get('row_data', {})
        
        # === EXTRA√á√ÉO DE URLs V√ÅLIDAS ===
        # Tenta m√∫ltiplas fontes para garantir que captura URLs
        urls_fontes = []
        
        # Primeiro tenta pegar das sources (fontes citadas)
        for source in data.get('sources', []):
            url = source.get('url', '')
            if url and url.startswith('http'):
                urls_fontes.append(url)
        
        # Se n√£o encontrou, tenta nos raw_findings
        if not urls_fontes:
            for finding in data.get('raw_findings', []):
                url = finding.get('link', '') or finding.get('source', '')
                if url and url.startswith('http') and url not in urls_fontes:
                    urls_fontes.append(url)
        
        # === CONSTRU√á√ÉO DA LINHA DO CSV ===
        csv_row = {
            'processo': cod,
            'titular': row_info.get(COL_TITULAR, ''),
            'uf': row_info.get(COL_UF, ''),
            'area_hectares': row_info.get('area_ha_calculada', 0),
            'resumo_analise': data.get('summary', '').replace('\n', ' ').strip(),  # Remove quebras de linha
            'possui_impacto_mencionado': 'Sim' if cod in processos_com_impacto else 'N√£o',
            'num_fontes_consultadas': len(urls_fontes),
            'fontes_urls': '; '.join(urls_fontes),  # Separa URLs com ponto-v√≠rgula
            'data_analise': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'motor_busca': SEARCH_ENGINE_USED
        }
        
        # === EXTRA√á√ÉO DE DESCOBERTAS RELEVANTES ===
        # Pega os primeiros 200 caracteres das 3 descobertas mais relevantes
        descobertas = []
        for idx, finding in enumerate(data.get('raw_findings', [])[:3]):
            descobertas.append(finding.get('content', '')[:200])
        csv_row['descobertas_relevantes'] = ' | '.join(descobertas)  # Separa com pipe
        
        csv_data.append(csv_row)
    
    # === CRIA√á√ÉO E SALVAMENTO DO DATAFRAME ===
    df_resultados = pd.DataFrame(csv_data)
    csv_filename = os.path.join(OUTPUT_DIR, "analise_sigmine_resultados.csv")
    
    # encoding='utf-8-sig' adiciona BOM para compatibilidade com Excel
    df_resultados.to_csv(csv_filename, index=False, encoding='utf-8-sig')
    
    # === GERA√á√ÉO DO CSV DETALHADO DE DESCOBERTAS ===
    # Este arquivo cont√©m todas as descobertas com impactos, n√£o resumidas
    descobertas_data = []
    
    for cod, data in contexto_proc.items():
        for finding in data.get('raw_findings', []):
            # Extrai URL v√°lida
            url_encontrada = finding.get('link', '') or finding.get('source', '')
            if not url_encontrada.startswith('http'):
                url_encontrada = ''
            
            # Cria registro detalhado
            descobertas_data.append({
                'processo': cod,
                'titular': data.get('row_data', {}).get(COL_TITULAR, ''),
                'conteudo_descoberta': finding.get('content', ''),  # Conte√∫do completo
                'fonte_url': url_encontrada,
                'titulo_fonte': finding.get('title', ''),
                'query_busca': finding.get('query', ''),  # Query que encontrou o resultado
                'site_relevante': 'Sim' if finding.get('is_relevant_site', False) else 'N√£o',
                'motor_busca': SEARCH_ENGINE_USED
            })
    
    # Salva CSV de descobertas se houver dados
    if descobertas_data:
        df_descobertas = pd.DataFrame(descobertas_data)
        descobertas_filename = os.path.join(OUTPUT_DIR, "descobertas_impactos_detalhadas.csv")
        df_descobertas.to_csv(descobertas_filename, index=False, encoding='utf-8-sig')
        print(f"‚úÖ Descobertas detalhadas salvas em: '{descobertas_filename}'")
    
    # === MENSAGENS FINAIS ===
    print(f"\n‚úÖ Relat√≥rio final salvo em: '{REPORT_FILENAME}'")
    print(f"‚úÖ Resultados em CSV salvos em: '{csv_filename}'")
    print(f"‚úÖ Motor de busca utilizado: {SEARCH_ENGINE_USED}")
    print("\nüéâ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
    print("=" * 60)

# === PONTO DE ENTRADA DO PROGRAMA ===
# Este bloco s√≥ executa se o script for rodado diretamente
# N√£o executa se for importado como m√≥dulo
if __name__ == "__main__":
    main()